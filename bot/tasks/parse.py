import asyncio
import io
import logging
from datetime import datetime, timezone
from typing import List, Dict, Optional, Union

import aiohttp
import pandas as pd
from aiogram import Bot
from aiogram.types import BufferedInputFile
from openpyxl.utils import get_column_letter
from parsel import Selector
from tortoise.transactions import in_transaction

from bot.database.models.price_history import PriceHistory
from bot.database.models.product_group import ProductGroup
from core.config import load_config

logger = logging.getLogger(__name__)
config = load_config()
bot = Bot(token = config.tg_bot.token)

import time


def format_progress(start_time: float, current: int, total: int) -> str:
    # –ü—Ä–æ—Ü–µ–Ω—Ç—ã
    percent = int((current / total) * 100) if total > 0 else 0
    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    bar_length = 10
    filled_length = int(bar_length * current // total) if total > 0 else 0
    bar = "‚ñà" * filled_length + "‚ñë" * (bar_length - filled_length)

    # –í—Ä–µ–º—è
    elapsed = int(time.time() - start_time)
    elapsed_str = time.strftime("%M–º %Ss", time.gmtime(elapsed))

    # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è
    if current > 0:
        estimated_total = elapsed * total // current
        remaining = estimated_total - elapsed
    else:
        remaining = 0
    remaining_str = time.strftime("%M–º %Ss", time.gmtime(remaining))

    # –§–æ—Ä–º–∞—Ç —Ç–µ–∫—Å—Ç–∞
    return (
        f"‚è± –í—Ä–µ–º—è: {elapsed_str}\n"
        f"üì¶ –ü—Ä–æ–≥—Ä–µ—Å—Å: [{bar}] {percent}% ({current}/{total})\n"
        f"‚è≥ –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ: {remaining_str}"
    )


class ProductParser:
    MAX_RETRIES = 3
    REQUEST_DELAY = 0.1  # seconds

    def __init__(self, session: aiohttp.ClientSession):
        self.session = session

    async def fetch(self, url: str) -> Optional[str]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        for attempt in range(self.MAX_RETRIES):
            try:
                async with self.session.get(url, timeout = aiohttp.ClientTimeout(total = 10)) as response:
                    response.raise_for_status()
                    return await response.text()
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                logger.warning(f"[{url}] –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < self.MAX_RETRIES - 1:
                    await asyncio.sleep(self.REQUEST_DELAY * (attempt + 1))
        logger.error(f"[{url}] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ {self.MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫")
        return None

    @staticmethod
    def extract(selector: Selector, xpath: str, default: str = "") -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ XPath."""
        try:
            value = selector.xpath(xpath).get()
            return value.strip() if value else default
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ XPath '{xpath}': {e}")
            return default

    async def parse_product(self, url: str) -> Optional[Dict[str, Union[str, float]]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–¥—É–∫—Ç–∞."""
        html = await self.fetch(url)
        if not html:
            return None

        selector = Selector(html)
        return {
            "link": url,
            "title": self.extract(selector, "//h1[@data-qaid='product_name']/text()"),
            "price": self.extract(selector, "//div[@class='tqUsL']//div/@data-qaprice"),
            "company": self.extract(selector, "//div[@class='l-GwW fvQVX']/a[@data-qaid='company_name']/text()"),
        }


async def generate_excel(data: List[Dict]) -> io.BytesIO:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel-—Ñ–∞–π–ª–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π."""
    df = pd.DataFrame(data)
    output = io.BytesIO()

    with pd.ExcelWriter(output, engine = "openpyxl") as writer:
        df.to_excel(writer, index = False, sheet_name = "–û—Ç—á—ë—Ç")
        worksheet = writer.sheets["–û—Ç—á—ë—Ç"]

        # –ê–≤—Ç–æ—à–∏—Ä–∏–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
        for col in worksheet.columns:
            max_length = max((len(str(cell.value)) for cell in col), default = 10)
            worksheet.column_dimensions[get_column_letter(col[0].column)].width = (max_length + 2) * 1.2

    output.seek(0)
    return output


async def process_group(group: ProductGroup, parser: ProductParser, with_stop_button: bool = False):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Å—ã–ª–æ–∫ –≥—Ä—É–ø–ø—ã, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ Excel –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""
    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥—Ä—É–ø–ø—É '{group.title}' (id={group.id})")
    data = []
    total_links = len(group.product_links)
    parsed_links = 0
    last_text = None
    start_time = time.time()

    async with in_transaction() as conn:
        for idx, link in enumerate(group.product_links, start = 1):
            product = await parser.parse_product(link.url)
            if not product:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å {link.url}")
                continue

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–µ–π
            link.productName = product.get("title") or link.productName
            link.companyName = product.get("company") or link.companyName

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–Ω—ã
            raw_price = product.get("price")

            try:
                price_value = float("".join(ch for ch in raw_price if ch.isdigit() or ch == ".")) if raw_price else 0.0
            except ValueError:
                price_value = 0.0

            link.last_price = price_value
            link.last_check = datetime.now(timezone.utc)
            await link.save(using_db = conn)

            await PriceHistory.create(
                product_link = link,
                price = int(price_value),
                date = datetime.now(timezone.utc),
                using_db = conn
            )

            data.append(
                {
                    "–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏": link.last_check.strftime("%d.%m.%Y"),
                    "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞": link.productName,
                    "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏": link.companyName,
                    "–°—Ç–æ–∏–º–æ—Å—Ç—å": link.last_price,
                    "–°—Å—ã–ª–∫–∞": link.url,
                }
            )
            parsed_links += 1

            # üî¥ –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = format_progress(start_time, idx, total_links)
            new_text = f"–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥—Ä—É–ø–ø—ã: {group.title}\n{progress_bar}"

            # –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –≤ –¢–µ–ª–µ–≥—Ä–∞–º ‚Äî —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π —Å–æ–æ–±—â–µ–Ω–∏–µ
            if group.user.telegram_id:
                try:
                    if idx == 1:  # –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
                        msg = await bot.send_message(
                            group.user.telegram_id, f"–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥—Ä—É–ø–ø—ã: {group.title}\n{progress_bar}",
                        )
                    else:  # –¥–∞–ª—å—à–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º
                        if new_text != last_text:  # üî¥ –ø—Ä–æ–≤–µ—Ä—è–µ–º
                            await bot.edit_message_text(
                                chat_id = group.user.telegram_id,
                                message_id = msg.message_id,
                                text = new_text,
                            )
                            last_text = new_text
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: {e}")

    if data and group.user.telegram_id:
        excel_file = await generate_excel(data)
        await bot.send_document(
            chat_id = group.user.telegram_id,
            document = BufferedInputFile(excel_file.getvalue(), filename = f"{group.title}.xlsx"),
            caption = (
                f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω.\n–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {total_links}\n–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {parsed_links}\n\n"
                f"–û—Ç—á—ë—Ç –ø–æ –≥—Ä—É–ø–ø–µ: {group.title}"
            )
        )
        logger.info(f"–û—Ç—á—ë—Ç –ø–æ –≥—Ä—É–ø–ø–µ '{group.title}' –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {group.user.telegram_id}")


async def parse_all_groups():
    """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –ø–æ –≤—Å–µ–º –∞–∫—Ç–∏–≤–Ω—ã–º –≥—Ä—É–ø–ø–∞–º."""
    logger.info("–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")
    async with aiohttp.ClientSession() as session:
        parser = ProductParser(session)
        groups = await ProductGroup.filter(is_active = True).select_related("user").prefetch_related("product_links")

        if not groups:
            logger.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä—É–ø–ø –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return

        for group in groups:
            await process_group(group, parser)

    logger.info("–§–æ–Ω–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω ‚úÖ")


async def parse_single_group(group_id: int):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã."""
    group = await ProductGroup.get(id = group_id).select_related("user").prefetch_related("product_links")

    if not group:
        logger.warning(f"–ì—Ä—É–ø–ø–∞ —Å id={group_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return

    async with aiohttp.ClientSession() as session:
        parser = ProductParser(session)
        await process_group(group, parser)

    logger.info(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≥—Ä—É–ø–ø—ã '{group.title}' (id={group.id}) –∑–∞–≤–µ—Ä—à—ë–Ω ‚úÖ")
